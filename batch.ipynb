{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f065fbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastcoref gdown torch transformers\n",
    "!python -m spacy download en_core_web_trf\n",
    "!pip install spacy[transformers]\n",
    "!git clone https://github.com/Ahmed-Khaled-Saleh/npat.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb42775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import os\n",
    "url_json = \"https://drive.google.com/file/d/18A78dkzkfgTTPVjvsueDmIRxKUx4YDDX/view?usp=sharing\"\n",
    "os.makedirs(\"./data\", exist_ok= True)\n",
    "output = \"data/cleaned_economics_data.json\"\n",
    "gdown.download(url_json, output= output, fuzzy= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a26525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./data/cleaned_economics_data.json', 'r') as f:\n",
    "    articles = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f293604a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1186"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6fd8f4",
   "metadata": {},
   "source": [
    "## Quotation Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2793b99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"https://drive.google.com/file/d/11yCgOGHXJuT3-7sENdRR2Zeu8r3pR5cK/view?usp=drive_link\"\n",
    "model_path = os.path.join(\".\",\"src/cofenet/checkpoint/model_6000.bin\")\n",
    "gdown.download(checkpoint, output= model_path, fuzzy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77db82da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.cofenet.model.mod_bert import ModelBert_Cofe\n",
    "import torch\n",
    "\n",
    "model_cofe = ModelBert_Cofe()\n",
    "state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "model_cofe.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f38b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.cofenet.utils.utils import *\n",
    "from src.cofenet.utils.loader import SingleDataLoader\n",
    "from src.cofenet.utils.dataset import DatasetBert\n",
    "from torch.utils.data import SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ad956ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tgidss2tgstrss(tgidss, tags_file_path ,lengths=None):\n",
    "    tgstrss = []\n",
    "    map_tg2tgid = {tag: idx for idx, tag in enumerate(load_text_file_by_line(tags_file_path))}\n",
    "    map_tgid2tg = {idx: tag for tag, idx in map_tg2tgid.items()}\n",
    "    \n",
    "    if lengths is None:\n",
    "        for tgids in tgidss:\n",
    "            tgstrss.append([map_tgid2tg[tgid] for tgid in tgids])\n",
    "    else:\n",
    "        for tgids, length in zip(tgidss, lengths):\n",
    "            tgstrss.append([map_tgid2tg[tgid] for tgid in tgids[:length]])\n",
    "    return tgstrss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "def read_data(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        raise Exception('data file_path is not exist')\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file_object:  \n",
    "        data = json.load(file_object)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid\n",
    "def doc_text_preprocessing(doc):\n",
    "    global split_pargraphs\n",
    "    \n",
    "    def clean_text(txt):\n",
    "        txt = re.sub(r'(\\“|\\”)', \"\\\"\", txt)\n",
    "        txt = re.sub(r'[^a-zA-Z0-9 \\.\\'\\\"\\,\\-\\(\\)\\’\\$\\#\\@]', \"\", txt)\n",
    "        txt = re.sub(r'(\\( )', \"(\", txt)\n",
    "        txt = re.sub(r'( \\))', \")\", txt)\n",
    "        txt = re.sub(r'( \\.)', \".\", txt)\n",
    "        txt = re.sub(r'( \\,)', \",\", txt)\n",
    "        txt = re.sub(r'(.)\\.(.)', r'\\1. \\2', txt)\n",
    "        txt = re.sub(r' +', \" \", txt)\n",
    "        txt = re.sub(r'([a-z])\\.([a-z])', r'\\1 \\2', txt)\n",
    "        return txt.strip()\n",
    "\n",
    "    # clean text\n",
    "    doc['maintext'] = clean_text(doc['maintext'])\n",
    "\n",
    "    # check if there is no paragraphs to start split maintext\n",
    "    if len(doc['paragraphs']) == 0:\n",
    "       doc['paragraphs'] = split_pargraphs(doc['maintext'])\n",
    "\n",
    "    doc['paragraphs']  = list(filter(lambda x: len(x) > 1, map(lambda txt: clean_text(txt), doc['paragraphs'])))\n",
    "\n",
    "    # identify doc with id\n",
    "    doc['ID'] = str(uuid.uuid3(uuid.NAMESPACE_URL, doc['url']))\n",
    "    return doc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a787de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def handel_error(fun, doc):\n",
    "    try:\n",
    "        return fun(doc)\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "# data = read_data('./data/cleaned_economics_data.json')['data']\n",
    "docs = map(lambda doc: handel_error(doc_text_preprocessing, doc), articles['data'])\n",
    "docs = list(filter(lambda x: x != None, docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02252980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1186"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c55c525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmed/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from src.cofenet.utils.utils import *\n",
    "from src.cofenet.utils.loader import SingleDataLoader\n",
    "from src.cofenet.utils.dataset import DatasetBert\n",
    "from torch.utils.data import SequentialSampler\n",
    "\n",
    "\n",
    "def extract_quotes(infer_str:list, model_cofe) -> list:\n",
    "\n",
    "    file_path = read_write_str(infer_str, \"./src/cofenet/infer_file.txt\")\n",
    "    dataset = DatasetBert(file_path)\n",
    "    tag_file_path = './src/cofenet/utils/tag.txt'\n",
    "\n",
    "    dataloder = SingleDataLoader(dataset=dataset,\n",
    "                                batch_size=32,\n",
    "                                sampler=SequentialSampler(dataset),\n",
    "                                collate_fn=dataset.collate)\n",
    "    preds = []\n",
    "    for batch_data in dataloder:\n",
    "        model_cofe.eval()\n",
    "        with torch.no_grad():\n",
    "            batch_preds = model_cofe.predict(batch_data)\n",
    "            \n",
    "            batch_pred_strs = tgidss2tgstrss(\n",
    "                batch_preds.data.cpu().numpy() if not isinstance(batch_preds, list) else batch_preds, tag_file_path,\n",
    "                batch_data['lengths'].cpu().numpy())\n",
    "\n",
    "            preds.extend(batch_pred_strs)\n",
    "\n",
    "    os.remove(\"./src/cofenet/infer_file.txt\")\n",
    "    return preds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13d8a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quote_cue_source_extraction(doc):\n",
    "\n",
    "    def clean_text(txt):\n",
    "        txt = txt.lower()\n",
    "        # txt = re.sub(r'[^a-zA-Z0-9 \\.\\$\\#\\@]', \"\", txt)\n",
    "        # txt = re.sub(r' +', \" \", txt)\n",
    "        # txt = re.sub(r'(\\. |\\.$)', \" \", txt)\n",
    "        # txt = re.sub(r'([a-z])\\.([a-z])', r'\\1 \\2', txt)\n",
    "        return txt.strip()\n",
    "\n",
    "    paragraphs = doc['paragraphs']\n",
    "\n",
    "    # extract Cue, Source, and Quotes\n",
    "    predict_entities = extract_quotes(paragraphs, model_cofe)\n",
    "    return doc, predict_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101196f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = []\n",
    "for doc in docs:\n",
    "    try:\n",
    "        doc, predict_entities = quote_cue_source_extraction(doc)\n",
    "        ents.append(predict_entities)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document {doc['title']}: {e}\")\n",
    "        ents.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a4dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "output_filename = 'extracted_entities.json'\n",
    "\n",
    "with open(output_filename, 'w') as f:\n",
    "    json.dump(ents, f)\n",
    "\n",
    "print(f\"List 'ents' successfully saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce09d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./extracted_entities.json', 'r') as f:\n",
    "    ents = json.loads(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d0d3da",
   "metadata": {},
   "source": [
    "## Coref Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c87f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc, Span\n",
    "import spacy\n",
    "from typing import List\n",
    "from fastcoref import spacy_component\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "nlp.add_pipe(\"fastcoref\")\n",
    "\n",
    "def coref_resolver(txt, bio):\n",
    "    global nlp\n",
    "\n",
    "    def get_span_noun_indices(doc: Doc, cluster: List[List[int]]) -> List[int]:\n",
    "        spans = [doc[span[0]:span[1]+1] for span in cluster]\n",
    "        spans_pos = [[token.pos_ for token in span] for span in spans]\n",
    "        span_noun_indices = [i for i, span_pos in enumerate(spans_pos)\n",
    "            if any(pos in span_pos for pos in ['NOUN', 'PROPN'])]\n",
    "        return span_noun_indices\n",
    "\n",
    "    def get_cluster_head(doc: Doc, cluster: List[List[int]], noun_indices: List[int]):\n",
    "        head_idx = noun_indices[0]\n",
    "        head_start, head_end = cluster[head_idx]\n",
    "        head_span = doc[head_start:head_end+1]\n",
    "        return head_span, [head_start, head_end]\n",
    "\n",
    "    def is_containing_other_spans(span: List[int], all_spans: List[List[int]]):\n",
    "        return any([s[0] >= span[0] and s[1] <= span[1] and s != span for s in all_spans])\n",
    "\n",
    "    def improved_replace_corefs(document, clusters):\n",
    "        all_spans = [span for cluster in clusters for span in cluster]\n",
    "        coref_results = []\n",
    "        for cluster in clusters:\n",
    "            noun_indices = get_span_noun_indices(document, cluster)\n",
    "            if noun_indices:\n",
    "                mention_span, mention = get_cluster_head(document, cluster, noun_indices)\n",
    "                for coref in cluster:\n",
    "                    if coref != mention and not is_containing_other_spans(coref, all_spans):\n",
    "                        coref_results.append({\"coref_pos\": coref, \"refer\": mention_span.text})\n",
    "        return coref_results\n",
    "\n",
    "    try:\n",
    "        doc = nlp(txt)\n",
    "        coref_clusters = doc._.coref_clusters\n",
    "        clusters = []\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        for cluster in coref_clusters:\n",
    "            spans = []\n",
    "            \n",
    "            for mention in cluster:\n",
    "                start = mention[0]\n",
    "                end = mention[1] - 1  # match AllenNLP indexing\n",
    "                spans.append([start, end])\n",
    "            clusters.append(spans)\n",
    "    except Exception as e:\n",
    "        print(\"*\"*20)\n",
    "        print(f\"Error processing: {txt}\")\n",
    "        print(e)\n",
    "        print(\"*\"*20)\n",
    "        return []\n",
    "\n",
    "    # build spaCy doc with BIO entities\n",
    "    words = txt.split(' ')\n",
    "    spaces = [True] * len(words)\n",
    "    doc_ = Doc(nlp.vocab, words=words, spaces=spaces, ents=bio)\n",
    "    doc = nlp(doc_)\n",
    "    coref_results = improved_replace_corefs(doc, clusters)\n",
    "    return coref_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d6d418",
   "metadata": {},
   "outputs": [],
   "source": [
    "coref_res = []\n",
    "for idx, doc in enumerate(docs):\n",
    "    for j, p in enumerate(doc['paragraphs']):\n",
    "        coref_res.append(coref_resolver(p, ents[idx][j]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb386a8",
   "metadata": {},
   "source": [
    "## Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b120083",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def enhance_source(span, enhance_if_large_than=4):\n",
    "    global nlp\n",
    "    position = [span.start, span.end]\n",
    "    doc = nlp(Doc(nlp.vocab, words=span.text.split(' ')))\n",
    "    if len(doc) <= enhance_if_large_than: \n",
    "        return span.text, position\n",
    "\n",
    "    # get first person entity\n",
    "    ent = list(filter(lambda x: x.label_ == 'PERSON', doc.ents))[0]\n",
    "\n",
    "    left = []\n",
    "    for i in range(1,6):\n",
    "        tok = doc[ent.start-i]\n",
    "        if ent.start-i < 0 or tok.pos_ not in {\"PROPN\", \"PRON\"}: break\n",
    "        left.append(tok.text)\n",
    "\n",
    "    right = []\n",
    "    for i in range(1,6):\n",
    "        if ent.start+i > len(doc)-1 or tok.pos_ not in {\"PROPN\", \"PRON\"}: break\n",
    "        tok = doc[ent.start+i]\n",
    "        right.append(tok.text)\n",
    "\n",
    "\n",
    "    local_pos = [ent.start - len(left), ent.end + len(right)]\n",
    "    position[0] = position[0] + local_pos[0]\n",
    "    position[1] = position[1] - (len(doc) - local_pos[1])\n",
    "    return doc[local_pos[0]: local_pos[1]].text.strip(), position\n",
    "\n",
    "\n",
    "\n",
    "def entity_linking(paragraphs, bios):\n",
    "    global nlp\n",
    "\n",
    "    linked_entities = []\n",
    "    for i, content in enumerate(zip(paragraphs, bios)):\n",
    "        local_linked_entities = []\n",
    "        # decompress the tuple\n",
    "        paragraph, bio = content\n",
    "\n",
    "        # the previous pargraph preparation\n",
    "        add_words, add_bio = [], []\n",
    "        # in case of first paragraph will ignore it, becouse there is no pargraphs before first one.\n",
    "        if i != 0:\n",
    "            # split to words\n",
    "            add_words = paragraphs[i-1].split(' ')\n",
    "            add_bio = bios[i-1]\n",
    "\n",
    "        # split to words and combine the previous paragraph with the current one.\n",
    "        words = add_words + paragraph.split(' ')\n",
    "        # prepare the spaces \n",
    "        spaces = [True]*len(words)\n",
    "        # combine the previous bio with the current one.\n",
    "        bio_ = add_bio + bio \n",
    "\n",
    "        # create Doc with its entities\n",
    "        doc_ = Doc(nlp.vocab, words=words, spaces=spaces, ents=bio_)\n",
    "        # feed the doc to default spacy pipeline to get the dependency tree and POS tags\n",
    "        doc = nlp(doc_)\n",
    "\n",
    "        # assigne the new doc ents with our entites.\n",
    "        doc.ents = doc_.ents\n",
    "\n",
    "        cues = list(filter(lambda ent: ent.label_ == 'cue', doc.ents)) # get list of cue-verb entities\n",
    "        sources = list(filter(lambda ent: ent.label_ == 'source', doc.ents)) # get list of source entities\n",
    "        contents = list(filter(lambda ent: ent.label_ == 'content', doc.ents)) # get list of content entities\n",
    "\n",
    "        # loop on each cue\n",
    "        for cue in cues:\n",
    "            # get only the verb word from cue, becaues cue and has many words\n",
    "            verb = None\n",
    "            verbs = list(filter(lambda tok: tok.pos_ =='VERB', cue))\n",
    "            if len(verbs) > 0:\n",
    "                verb = verbs[0] # get the first one\n",
    "\n",
    "            # get the source of cue based on POS & dependency tree\n",
    "            try:\n",
    "                source = None\n",
    "                # check all verb's children, if any one of them is labeled as a source entity.\n",
    "                source_part = next((child for child in verb.children if child.ent_type_ == 'source'), None)\n",
    "                # in case if no verb's children exist as source entity, look at the head \"Conj\" \n",
    "                if source_part == None:\n",
    "                    temp_verb = verb\n",
    "                    # loop untill get the source\n",
    "                    out_ = 0\n",
    "                    while temp_verb.dep_ != 'ROOT' or temp_verb.pos_ != 'VERB':\n",
    "                        temp_verb = temp_verb.head\n",
    "                        source_part = next((child for child in temp_verb.children if child.ent_type_ == 'source'), None)\n",
    "                        if out_ > 5: break\n",
    "                        out_+=1\n",
    "                \n",
    "                # get the original entity of source_part \n",
    "                for ent in sources:\n",
    "                    if source_part.i >= ent.start and source_part.i <= ent.end:\n",
    "                        source = ent\n",
    "                        break\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                quote = None\n",
    "                # check all verb's children, if any one of them is labeled as a content entity.\n",
    "                qoute_part = next((child for child in verb.children if child.ent_type_ == 'content'), None)\n",
    "                if qoute_part == None and verb.head.ent_type_ == 'content':\n",
    "                    qoute_part = verb.head\n",
    "\n",
    "                # get the original entity of qoute_part \n",
    "                for ent in contents:\n",
    "                    if qoute_part.i >= ent.start and qoute_part.i <= ent.end:\n",
    "                        quote = ent\n",
    "                        break\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                # to get only quotes from the current paragraph, and not include quotes from the previous one.\n",
    "                if quote[0].i >= len(add_words):\n",
    "                    enhanced_source, position = enhance_source(source)\n",
    "                    obj = {\"Speaker\": enhanced_source,\n",
    "                           \"Speaker_position\": position, # useing in corfrence resolution \n",
    "                           \"Cue\": verb.text,\n",
    "                           \"Quote\": quote.text,\n",
    "                        #    \"Quote_polarity\": get_polarity([quote.text])[0],\n",
    "                        #    \"Quote_summarization\": summerizer_model.predict(quote.text)[0] if len(quote.text.split(' ')) > 20 else quote.text\n",
    "                    }\n",
    "                    local_linked_entities.append(obj)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        linked_entities.append(local_linked_entities)\n",
    "    return linked_entities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c465ab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_ents = []\n",
    "for idx, doc in enumerate(docs):\n",
    "    try:\n",
    "        linked_en = entity_linking(doc['paragraphs'], ents[idx])\n",
    "        linked_ents.append(linked_en)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document {doc['title']}: {e}\")\n",
    "        linked_ents.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405cdcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_coref_on_linked_entities(linking_out, coref_out):\n",
    "    linking_qouts = []  \n",
    "    for paragraph_links, coref_links in zip(linking_out, coref_out):\n",
    "        local_linkes = []\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        for quote in paragraph_links:\n",
    "            print(coref_links)\n",
    "            print(quote)\n",
    "            source_start, source_end = quote['Speaker_position'][0], quote['Speaker_position'][1]-1  \n",
    "\n",
    "            new_source = list(filter(lambda x: x['coref_pos'][0] <= source_start and x['coref_pos'][1] >= source_end , coref_links))\n",
    "            new_link = {k:v for k, v in quote.items() if k != 'Speaker_position'}\n",
    "            if len(new_source) > 0:\n",
    "                new_link['Speaker'] = new_source[0]['refer']\n",
    "            \n",
    "            local_linkes.append(new_link)\n",
    "         \n",
    "        linking_qouts.append(local_linkes)\n",
    "    return linking_qouts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63980aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_enents_coref = []\n",
    "for idx in range(len(docs)):\n",
    "    try:\n",
    "        linked_en = apply_coref_on_linked_entities(linked_ents[idx], coref_res[idx])\n",
    "        linked_enents_coref.append(linked_en)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document {docs[idx]['title']}: {e}\")\n",
    "        linked_enents_coref.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd3553e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdd9926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
